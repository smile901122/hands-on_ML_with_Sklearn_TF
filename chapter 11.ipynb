{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用 Tensorflow 来实现批量归一化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import fully_connected\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "bn_params = {\n",
    "    'is_training': is_training,\n",
    "    'decay': 0.99,\n",
    "    'updates_collections': None,\n",
    "}\n",
    "\n",
    "hidden1 = fully_connected(X, n_hidden1, scope='hidden1', normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2', normalizer_fn=batch_norm, normalizer_params=bn_params)\n",
    "logits = fully_connected(hidden2, n_outputs, scope='outputs', activation_fn=None, normalizer_fn=batch_norm, normalizer_params=bn_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 利用 arg_scope() 方法构造一个参数范围：第一个参数是一个函数列表，其他参数会自动传给这些函数\n",
    "\n",
    "with tf.contrib.framework.arg_scope([fully_connected], normalizer_fn=batch_norm, normalizer_params=bn_params):\n",
    "    hidden1 = fully_connected(X, n_hidden1, scope='hidden1', reuse=tf.AUTO_REUSE)\n",
    "    hidden2 = fully_connected(hidden1, n_hidden2, scope='hidden2', reuse=tf.AUTO_REUSE)\n",
    "    logits = fully_connected(hidden2, n_outputs, scope='outputs', activation_fn=None, reuse=tf.AUTO_REUSE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-3-eb8a235f51e0>:13: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.dense instead.\n",
      "WARNING:tensorflow:From <ipython-input-3-eb8a235f51e0>:14: batch_normalization (from tensorflow.python.layers.normalization) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use keras.layers.batch_normalization instead.\n"
     ]
    }
   ],
   "source": [
    "# contrib 尽量少使用\n",
    "# 由于tf.layers.dense()函数与本书中使用的tf.contrib.layers.arg_scope()不兼容，我们现在使用 python 的functools.partial()函数\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "X = tf.placeholder(tf.float32, shape=(None, n_inputs), name=\"X\")\n",
    "y = tf.placeholder(tf.int64, shape=None, name = 'y')\n",
    "training = tf.placeholder_with_default(False, shape=(), name='training')\n",
    "\n",
    "my_batch_norm_layer = partial(tf.layers.batch_normalization,\n",
    "                              training=training, momentum=0.9)\n",
    "\n",
    "hidden1 = tf.layers.dense(X, n_hidden1, name=\"hidden1\", reuse=tf.AUTO_REUSE)\n",
    "bn1 = my_batch_norm_layer(hidden1)\n",
    "bn1_act = tf.nn.elu(bn1)\n",
    "hidden2 = tf.layers.dense(bn1_act, n_hidden2, name=\"hidden2\", reuse=tf.AUTO_REUSE)\n",
    "bn2 = my_batch_norm_layer(hidden2)\n",
    "bn2_act = tf.nn.elu(bn2)\n",
    "logits_before_bn = tf.layers.dense(bn2_act, n_outputs, name=\"outputs\", reuse=tf.AUTO_REUSE)\n",
    "logits = my_batch_norm_layer(logits_before_bn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "完整代码如下："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-2c3bed79590d>:11: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "0 Test accuracy: 0.8869\n",
      "1 Test accuracy: 0.9086\n",
      "2 Test accuracy: 0.9204\n",
      "3 Test accuracy: 0.929\n",
      "4 Test accuracy: 0.9374\n",
      "5 Test accuracy: 0.9419\n",
      "6 Test accuracy: 0.9475\n",
      "7 Test accuracy: 0.9498\n",
      "8 Test accuracy: 0.9549\n",
      "9 Test accuracy: 0.9567\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "n_inputs = 28 * 28\n",
    "n_hidden1 = 300\n",
    "n_hidden2 = 100\n",
    "n_outputs = 10\n",
    "\n",
    "mnist = input_data.read_data_sets(\"data/\")\n",
    "\n",
    "batch_norm_momentum = 0.9\n",
    "learning_rate = 0.01\n",
    "\n",
    "# X = tf.placeholder(tf.float32, shape=(None, n_inputs), name = 'X')\n",
    "# y = tf.placeholder(tf.int64, shape=None, name = 'y')\n",
    "# training = tf.placeholder_with_default(False, shape=(), name = 'training')#给Batch norm加一个placeholder\n",
    "\n",
    "with tf.name_scope(\"dnn\"):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    #对权重的初始化\n",
    "\n",
    "    my_batch_norm_layer = partial(\n",
    "        tf.layers.batch_normalization,\n",
    "        training = training,\n",
    "        momentum = batch_norm_momentum\n",
    "    )\n",
    "\n",
    "    my_dense_layer = partial(\n",
    "        tf.layers.dense,\n",
    "        kernel_initializer = he_init\n",
    "    )\n",
    "\n",
    "    hidden1 = my_dense_layer(X ,n_hidden1 ,name = 'hidden1', reuse=tf.AUTO_REUSE)\n",
    "    bn1 = tf.nn.elu(my_batch_norm_layer(hidden1))\n",
    "    hidden2 = my_dense_layer(bn1, n_hidden2, name = 'hidden2', reuse=tf.AUTO_REUSE)\n",
    "    bn2 = tf.nn.elu(my_batch_norm_layer(hidden2))\n",
    "    logists_before_bn = my_dense_layer(bn2, n_outputs, name = 'outputs', reuse=tf.AUTO_REUSE)\n",
    "    logists = my_batch_norm_layer(logists_before_bn)\n",
    "\n",
    "with tf.name_scope('loss'):\n",
    "    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels = y, logits= logists)\n",
    "    loss = tf.reduce_mean(xentropy, name = 'loss')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "    training_op = optimizer.minimize(loss)\n",
    "\n",
    "with tf.name_scope(\"eval\"):\n",
    "    correct = tf.nn.in_top_k(logists, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "\n",
    "init = tf.global_variables_initializer()\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "n_epoches = 10\n",
    "batch_size = 200\n",
    "# 注意：由于我们使用的是 tf.layers.batch_normalization() 而不是 tf.contrib.layers.batch_norm()（如本书所述），\n",
    "# 所以我们需要明确运行批量规范化所需的额外更新操作（sess.run([ training_op，extra_update_ops], ...)。\n",
    "extra_update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    for epoch in range(n_epoches):\n",
    "        for iteraton in range(mnist.train.num_examples // batch_size):\n",
    "            X_batch, y_batch = mnist.train.next_batch(batch_size)\n",
    "            sess.run([training_op, extra_update_ops], feed_dict={training: True, X: X_batch, y: y_batch})\n",
    "        accuracy_val = accuracy.eval(feed_dict= {X: mnist.test.images,\n",
    "                                                y: mnist.test.labels})\n",
    "        print(epoch, 'Test accuracy:', accuracy_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 梯度剪裁"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "一般情况下，大家倾向于批量归一化，但是梯度裁剪对于 RNN 网络非常有效，可以减轻梯度爆炸问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "optimizer = tf.train.GradientDescentOptimizer(learning_rate=learning_rate)\n",
    "grads_and_vars = optimizer.compute_gradients(loss)\n",
    "capped_gvs = [(tf.clip_by_value(grad, -threshold, threshold), var) for grad, var in grads_and_vars]\n",
    "training_op = optimizer.apply_gradients(capped_gvs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重用预训练图层"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从头开始训练一个非常庞大的 DNN 并不明智，大多数时候应该试着找一个能处理相似问题的已有的神经网络，然后重用它的低层网络，这叫做迁移学习。这不仅能极大地提升训练速度，也很大程度地减少训练数据。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 重用 TensorFlow 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "通常我们只想要重用原来模型的一部分，一种简单的解决方案就是配置Saver使之在还原的时候后值还原所有参数的一个子集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
